{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHiddenLayerNN():   \n",
    "    \n",
    "     def __init__(self, d,d1): \n",
    "        \n",
    "        \"\"\"\n",
    "        d = number of neurons in the input vector/layer\n",
    "        d1 = number of neurons in the first hidden layer\n",
    "        the network outputs single value\n",
    "        \n",
    "        \"\"\"\n",
    "        # hyperparameters\n",
    "        self.d=d     \n",
    "        self.d1=d1     \n",
    "        # weights\n",
    "        self.W1=2*np.random.uniform(0,1,(self.d1,self.d))-1\n",
    "        self.W2=2*np.random.uniform(0,1,(1,self.d1))-1  \n",
    "        # biases\n",
    "        self.b1=2*np.random.uniform(0,1,self.d1)-1 \n",
    "        self.b2=2*np.random.uniform(0,1)-1      \n",
    "        \n",
    "     def forward_pass (self, minibatch):\n",
    "        size = minibatch.shape[0]\n",
    "        minibatch_values = np.zeros(size)\n",
    "            \n",
    "        net_layer1=np.dot(minibatch,self.W1.T)+self.b1\n",
    "        layer1 = np.maximum(net_layer1,0) # ReLU\n",
    "        minibatch_values = np.dot(layer1,self.W2.T) + self.b2 # the output point activation fuction is the identity function\n",
    "        \n",
    "        return minibatch_values, net_layer1, layer1\n",
    "          \n",
    "     def backpropagation(self, minibatch, targets):\n",
    "        \n",
    "        minibatch_values, net_layer1, layer1 = self.forward_pass(minibatch)\n",
    "        delta2 = 2*(minibatch_values - targets[:,np.newaxis])  \n",
    "        delta1 = delta2 @ self.W2    \n",
    "        \n",
    "        size = minibatch.shape[0]\n",
    "        grad_W2_C_f = (1/size)*np.sum((layer1).T @ delta2, axis=1)  \n",
    "        grad_W1_C_f = (1/size)*(minibatch.T @ (delta1 * np.where(net_layer1>=0,1,0) )) \n",
    "        grad_b2_C_f = (1/size)*np.sum(delta2)\n",
    "        grad_b1_C_f = (1/size)*np.sum(delta1 * np.where(net_layer1>=0,1,0), axis=0)\n",
    "         \n",
    "        return grad_W2_C_f, grad_W1_C_f, grad_b2_C_f, grad_b1_C_f\n",
    "    \n",
    "     def draw_random_minibatch(self, dataset, minibatch_size):\n",
    "         j = np.random.choice(dataset.shape[0],minibatch_size,False) \n",
    "         random_minibatch =  dataset[j]\n",
    "         return j, random_minibatch\n",
    "    \n",
    "     def sgd(self, x_train, y_train, learning_rate, minibatch_size, iterations) :\n",
    "        \n",
    "        for i in range(iterations):\n",
    "        \n",
    "            j,B = self.draw_random_minibatch(x_train, minibatch_size)\n",
    "            \n",
    "            f_B = self.forward_pass(B)\n",
    "            \n",
    "            gradW2, gradW1, gradb2, gradb1 = self.backpropagation(B, y_train[j]) \n",
    "            \n",
    "            self.W1 = self.W1 - learning_rate * gradW1.T\n",
    "            self.W2 = self.W2 - learning_rate * gradW2.T\n",
    "            self.b2 = self.b2 - learning_rate * gradb2\n",
    "            self.b1 = self.b1 - learning_rate * gradb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
